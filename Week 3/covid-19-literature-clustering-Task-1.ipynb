{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Literature Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red;\">You can find the full version of the interactive plot here on GitHub:</h2>\n",
    "\n",
    "### https://maksimekin.github.io/COVID19-Literature-Clustering/plots/t-sne_covid-19_interactive.html\n",
    "Because of its size, a sparse version is produced in this Notebook.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Cite This Work?\n",
    "```\n",
    "@inproceedings{COVID-19 Literature Clustering,\n",
    "\tauthor = {Eren, E. Maksim. Solovyev, Nick. Nicholas, Charles. Raff, Edward},\n",
    "\ttitle = {COVID-19 Literature Clustering},\n",
    "\tyear = {2020},\n",
    "\tmonth = {April},\n",
    "\tlocation = {University of Maryland Baltimore County (UMBC), Baltimore, MD, USA},\n",
    "\tnote={Malware Research Group},\n",
    "\turl = {\\url{https://github.com/MaksimEkin/COVID19-Literature-Clustering}},\n",
    "    howpublished = {TBA}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "Given the large number of literature and the rapid spread of COVID-19, it is difficult for health professionals to keep up with new information on the virus. Can clustering similar research articles together simplify the search for related publications? How can the content of the clusters be qualified?\n",
    "\n",
    "By using clustering for labelling in combination with dimensionality reduction for visualization, the collection of literature can be represented by a scatter plot. On this plot, publications of highly similar topic will share a label and will be plotted near each other. In order, to find meaning in the clusters, topic modelling will be performed to find the keywords of each cluster.\n",
    "\n",
    "By using Bokeh, the plot will be interactive. User’s will have the option of seeing the plot as a whole or filtering the data by cluster. If a narrower scope is required, the plot will also have a search function which will limit the output to only papers containing the search term. Hovering over points on the plot will give basic information like title, author, journal, and abstract. Clicking on a point will bring up a menu with a URL that can be used to access the full publication.\n",
    "\n",
    "This is a difficult time in which health care workers, sanitation staff, and many other essential personnel are out there keeping the world afloat. While adhering to quarantine protocol, the Kaggle CORD-19 competition has given us an opportunity to help in the best way we can as computer science students. It should be noted, however, that we are not epidemiologists, and it is not our place to gauge the importance of these papers. This tool was created to help make it easier for trained professionals to sift through many, many publications related to the virus, and find their own determinations.\n",
    "\n",
    "#### We welcome feedback so that we can continue to improve this project.\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach:\n",
    "\n",
    "- Parse the text from the body of each document using Natural Language Processing (NLP).\n",
    "- Turn each document instance $d_i$ into a feature vector $X_i$ using Term Frequency–inverse Document Frequency (TF-IDF).\n",
    "- Apply Dimensionality Reduction to each feature vector $X_i$ using t-Distributed Stochastic Neighbor Embedding (t-SNE) to cluster similar research articles in the two dimensional plane $X$ embedding $Y_1$.\n",
    "- Use Principal Component Analysis (PCA) to project down the dimensions of $X$ to a number of dimensions that will keep .95 variance while removing noise and outliers in embedding $Y_2$.\n",
    "- Apply k-means clustering on $Y_2$, where $k$ is 20, to label each cluster on $Y_1$.\n",
    "- Apply Topic Modeling on $X$ using Latent Dirichlet Allocation (LDA) to discover keywords from each cluster. \n",
    "- Investigate the clusters visually on the plot, zooming down to specific articles as needed, and via classification using Stochastic Gradient Descent (SGD). \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. Loading the data\n",
    "2. Pre-processing\n",
    "3. Vectorization\n",
    "4. PCA  & Clustering\n",
    "5. Dimensionality Reduction with t-SNE\n",
    "6. Topic Modeling on Each Cluster\n",
    "7. Classify\n",
    "8. Plot\n",
    "9. How to Use the Plot?\n",
    "10. Conclusion\n",
    "11. Citation/Sources\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "\n",
    ">In response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource of over 51,000 scholarly articles, including over 40,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up.\n",
    ">#### Cite: [COVID-19 Open Research Dataset Challenge (CORD-19) | Kaggle](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) \n",
    "> #### Kaggle Submission: [COVID-19 Literature Clustering | Kaggle](https://www.kaggle.com/maksimeren/covid-19-literature-clustering#Unsupervised-Learning:-Clustering-with-K-Means)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "Load the data following the notebook by Ivan Ega Pratama, from Kaggle.\n",
    "#### Cite: [Dataset Parsing Code | Kaggle, COVID EDA: Initial Exploration Tool](https://www.kaggle.com/ivanegapratama/covid-eda-initial-exploration-tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the metadata of the dateset. 'title' and 'journal' attributes may be useful later when we cluster the articles to see what kinds of articles cluster together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'CORD-19-research-challenge/'\n",
    "metadata_path = f'{root_path}/metadata.csv'\n",
    "meta_df = pd.read_csv(metadata_path, dtype={\n",
    "    'pubmed_id': str,\n",
    "    'Microsoft Academic Paper ID': str, \n",
    "    'doi': str\n",
    "})\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df[meta_df.pmcid=='PMC2750777']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch All of JSON File Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get path to all JSON files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\n",
    "len(all_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " File Reader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileReader:\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path) as file:\n",
    "            content = json.load(file)\n",
    "            self.paper_id = content['paper_id']\n",
    "            self.abstract = []\n",
    "            self.body_text = []\n",
    "            # Abstract\n",
    "            if 'abstract' in content.keys():\n",
    "                for entry in content['abstract']:\n",
    "                    self.abstract.append(entry['text'])\n",
    "            # Body text\n",
    "            for entry in content['body_text']:\n",
    "                self.body_text.append(entry['text'])\n",
    "            self.abstract = '\\n'.join(self.abstract)\n",
    "            self.body_text = '\\n'.join(self.body_text)\n",
    "    def __repr__(self):\n",
    "        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n",
    "first_row = FileReader(all_json[0])\n",
    "print(first_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function adds break after every words when character length reach to certain amount. This is for the interactive plot so that hover tool fits the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breaks(content, length):\n",
    "    data = \"\"\n",
    "    words = content.split(' ')\n",
    "    total_chars = 0\n",
    "\n",
    "    # add break every length characters\n",
    "    for i in range(len(words)):\n",
    "        total_chars += len(words[i])\n",
    "        if total_chars > length:\n",
    "            data = data + \"<br>\" + words[i]\n",
    "            total_chars = 0\n",
    "        else:\n",
    "            data = data + \" \" + words[i]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data into DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the helper functions, let's read in the articles into a DataFrame that can be used easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {'paper_id': [], 'doi':[], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\n",
    "for idx, entry in enumerate(all_json):\n",
    "    if idx % (len(all_json) // 10) == 0:\n",
    "        print(f'Processing index: {idx} of {len(all_json)}')\n",
    "    \n",
    "    try:\n",
    "        content = FileReader(entry)\n",
    "    except Exception as e:\n",
    "        continue  # invalid paper format, skip\n",
    "    \n",
    "    # get metadata information\n",
    "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
    "    # no metadata, skip this paper\n",
    "    if len(meta_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    dict_['abstract'].append(content.abstract)\n",
    "    dict_['paper_id'].append(content.paper_id)\n",
    "    dict_['body_text'].append(content.body_text)\n",
    "    \n",
    "\n",
    "    if len(content.abstract) == 0:\n",
    "        # no abstract provided\n",
    "        dict_['abstract_summary'].append(\"Not provided.\")\n",
    "    elif len(content.abstract.split(' ')) > 100:\n",
    "        # abstract provided is too long for plot, take first 100 words append with ...\n",
    "        info = content.abstract.split(' ')[:100]\n",
    "        summary = get_breaks(' '.join(info), 40)\n",
    "        dict_['abstract_summary'].append(summary + \"...\")\n",
    "    else:\n",
    "        # abstract is short enough\n",
    "        summary = get_breaks(content.abstract, 40)\n",
    "        dict_['abstract_summary'].append(summary)\n",
    "        \n",
    "    # get metadata information\n",
    "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
    "    \n",
    "    try:\n",
    "        # if more than one author\n",
    "        authors = meta_data['authors'].values[0].split(';')\n",
    "        if len(authors) > 2:\n",
    "            # if more than 2 authors, take them all with html tag breaks in between\n",
    "            dict_['authors'].append(get_breaks('. '.join(authors), 40))\n",
    "        else:\n",
    "            # authors will fit in plot\n",
    "            dict_['authors'].append(\". \".join(authors))\n",
    "    except Exception as e:\n",
    "        # if only one author - or Null valie\n",
    "        dict_['authors'].append(meta_data['authors'].values[0])\n",
    "    \n",
    "    # add the title information, add breaks when needed\n",
    "    try:\n",
    "        title = get_breaks(meta_data['title'].values[0], 40)\n",
    "        dict_['title'].append(title)\n",
    "    # if title was not provided\n",
    "    except Exception as e:\n",
    "        dict_['title'].append(meta_data['title'].values[0])\n",
    "    \n",
    "    # add the journal information\n",
    "    dict_['journal'].append(meta_data['journal'].values[0])\n",
    "    \n",
    "    # add doi\n",
    "    dict_['doi'].append(meta_data['doi'].values[0])\n",
    "    \n",
    "df_covid = pd.DataFrame(dict_, columns=['paper_id', 'doi', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some feature engineering\n",
    "Adding word count columns for both abstract and body_text can be useful parameters later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['abstract_word_count'] = df_covid['abstract'].apply(lambda x: len(x.strip().split()))  # word count in abstract\n",
    "df_covid['body_word_count'] = df_covid['body_text'].apply(lambda x: len(x.strip().split()))  # word count in body\n",
    "df_covid['body_unique_words']=df_covid['body_text'].apply(lambda x:len(set(str(x).split())))  # number of unique words in body\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['abstract'].describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Possible Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the unique values above, we can see that tehre are duplicates. It may have caused because of author submiting the article to multiple journals. Let's remove the duplicats from our dataset:\n",
    "\n",
    "(Thank you Desmond Yeoh for recommending the below approach on Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.drop_duplicates(['abstract', 'body_text'], inplace=True)\n",
    "df_covid['abstract'].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['body_text'].describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we didn't have duplicates. Instead, it was articles without Abstracts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a Look at the Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[Task 2: Add papers related to the chosen software from last weeK]__\n",
    "- get word count for abstract and body\n",
    "- get number of unique words for body\n",
    "- handle possible duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the majority of this notebook we will be working with **body_text** <br>\n",
    "Links to the papers will be generated using **doi** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, running the next steps of the notebook is not possible on the full dataset within Kaggle. The full plot is available is at https://maksimekin.github.io/COVID19-Literature-Clustering/plots/t-sne_covid-19_interactive.html.\n",
    "\n",
    "In Kaggle we will limit the dataframe to **10,000** instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_covid.sample(10000, random_state=42)\n",
    "del df_covid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset loaded, we need to clean-up the text to improve any clustering or classification efforts. First, let's drop Null vales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling multiple languages\n",
    "Next we are going to determine the language of each paper in the dataframe. Not all of the sources are English and the language needs to be identified so that we know how handle these instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "\n",
    "# set seed\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# hold label - language\n",
    "languages = []\n",
    "\n",
    "# go through each text\n",
    "for ii in tqdm(range(0,len(df))):\n",
    "    # split by space into list, take the first x intex, join with space\n",
    "    text = df.iloc[ii]['body_text'].split(\" \")\n",
    "    \n",
    "    lang = \"en\"\n",
    "    try:\n",
    "        if len(text) > 50:\n",
    "            lang = detect(\" \".join(text[:50]))\n",
    "        elif len(text) > 0:\n",
    "            lang = detect(\" \".join(text[:len(text)]))\n",
    "    # ught... beginning of the document was not in a good format\n",
    "    except Exception as e:\n",
    "        all_words = set(text)\n",
    "        try:\n",
    "            lang = detect(\" \".join(all_words))\n",
    "        # what!! :( let's see if we can find any text in abstract...\n",
    "        except Exception as e:\n",
    "            \n",
    "            try:\n",
    "                # let's try to label it through the abstract then\n",
    "                lang = detect(df.iloc[ii]['abstract_summary'])\n",
    "            except Exception as e:\n",
    "                lang = \"unknown\"\n",
    "                pass\n",
    "    \n",
    "    # get the language    \n",
    "    languages.append(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "languages_dict = {}\n",
    "for lang in set(languages):\n",
    "    languages_dict[lang] = languages.count(lang)\n",
    "    \n",
    "print(\"Total: {}\\n\".format(len(languages)))\n",
    "pprint(languages_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the language distribution in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['language'] = languages\n",
    "plt.bar(range(len(languages_dict)), list(languages_dict.values()), align='center')\n",
    "plt.xticks(range(len(languages_dict)), list(languages_dict.keys()))\n",
    "plt.title(\"Distribution of Languages in Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will be dropping any language that is not English. Attempting to translate foreign texts gave the following problems:\n",
    "\n",
    "1. API calls were limited\n",
    "\n",
    "2. Translating the language may not carry over the true semantic meaning of the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['language'] == 'en'] \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the spacy bio parser\n",
    "\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    !pip3.7 install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP \n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_sci_lg  # model downloaded in previous step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Part of the preprocessing will be finding and removing stopwords (common words that will act as noise in the clustering step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punctuations = string.punctuation\n",
    "stopwords = list(STOP_WORDS)\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the above stopwords are used in everyday english text. Research papers will often frequently use words that don't actually contribute to the meaning and are not considered everyday stopwords.\n",
    "\n",
    "Thank you Daniel Wolffram for the idea.\n",
    "#### Cite: [Custom Stop Words | Topic Modeling: Finding Related Articles](https://www.kaggle.com/danielwolffram/topic-modeling-finding-related-articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = [\n",
    "    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n",
    "    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n",
    "    'al.', 'Elsevier', 'PMC', 'CZI', 'www'\n",
    "]\n",
    "\n",
    "for w in custom_stop_words:\n",
    "    if w not in stopwords:\n",
    "        stopwords.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next lets create a function that will process the text data for us. \n",
    "For this purpose we will be using the spacy library. This function will convert text to lower case, remove punctuation, and find and remove stopwords. For the parser, we will use en_core_sci_lg. This is a model for processing biomedical, scientific or clinical text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser\n",
    "parser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\n",
    "parser.max_length = 7000000\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n",
    "    mytokens = \" \".join([i for i in mytokens])\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the text-processing function on the **body_text**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "df[\"processed_text\"] = df[\"body_text\"].progress_apply(spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at word count in the papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['body_word_count'])\n",
    "df['body_word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['body_unique_words'])\n",
    "df['body_unique_words'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two plots give us a good idea of the content we are dealing with. Most papers are about 5000 words in length. The long tails in both plots are caused by outliers. In fact, ~98% of the papers are under 20,000 words in length while a select few are over 200,000! <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "\n",
    "Now that we have pre-processed the data, it is time to convert it into a format that can be handled by our algorithms. For this purpose we will be using tf-idf. This will convert our string formatted data into a measure of how important each word is to the instance out of the literature as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def vectorize(text, maxx_features):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=maxx_features)\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize our data. We will be clustering based off the content of the body text. The maximum number of features will be limited. Only the top 2 ** 12 features will be used, eseentially acting as a noise filter. Additionally, more features cause painfully long runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['processed_text'].values\n",
    "X = vectorize(text, 2 ** 12)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA  & Clustering\n",
    "\n",
    "Let's see how much we can reduce the dimensions while still keeping 95% variance. We will apply Principle Component Analysis (PCA) to our vectorized data. The reason for this is that by keeping a large number of dimensions with PCA, you don’t destroy much of the information, but hopefully will remove some noise/outliers from the data, and make the clustering problem easier for k-means. Note that X_reduced will only be used for k-means, t-SNE will still use the original feature vector X that was generated through tf-idf on the NLP processed text.\n",
    "\n",
    "(Thank you Dr. Edward Raff for the suggestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_reduced= pca.fit_transform(X.toarray())\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To separate the literature, k-means will be run on the vectorized text. Given the number of clusters, k, k-means will categorize each vector by taking the mean distance to a randomly initialized centroid. The centroids are updated iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[source](https://en.wikipedia.org/wiki/K-means_clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many clusters? \n",
    "\n",
    "To find the best k value for k-means we'll look at the distortion at different k values. Distortion computes the sum of squared distances from each point to its assigned center. When distortion is plotted against k there will be a k value after which decreases in distortion are minimal. This is the desired number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# run kmeans with many different k\n",
    "distortions = []\n",
    "K = range(2, 50)\n",
    "for k in K:\n",
    "    k_means = KMeans(n_clusters=k, random_state=42).fit(X_reduced)\n",
    "    k_means.fit(X_reduced)\n",
    "    distortions.append(sum(np.min(cdist(X_reduced, k_means.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "    #print('Found distortion for {} clusters'.format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_line = [K[0], K[-1]]\n",
    "Y_line = [distortions[0], distortions[-1]]\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'b-')\n",
    "plt.plot(X_line, Y_line, 'r')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot we can see that the better k values are between 18-25. After that, the decrease in distortion is not as significant. For simplicity, we will use k=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an appropriate k value, we can run k-means on the PCA-processed feature vector (X_reduced). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X_reduced)\n",
    "df['y'] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction with t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [t-SNE](https://lvdmaaten.github.io/tsne) we can reduce our high dimensional features vector to 2 dimensions. By using the 2 dimensions as x,y coordinates, the body_text can be plotted. \n",
    "\n",
    "> t-Distributed Stochastic Neighbor Embedding (t-SNE) reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space\n",
    "> #### Cite: [Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow: Second Edition | Aurélien Geron](https://github.com/ageron/handson-ml2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(verbose=1, perplexity=100, random_state=42)\n",
    "X_embedded = tsne.fit_transform(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that step took a while! Let's take a look at what our data looks like when compressed to 2 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# colors\n",
    "palette = sns.color_palette(\"bright\", 1)\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\n",
    "plt.title('t-SNE with no Labels')\n",
    "plt.savefig(\"Data_Collection/Task_1/t-sne_covid19_Task1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty bland. There are some clusters we can immediately detect, but the many instances closer to the center are harder to separate. t-SNE did a good job at reducing the dimensionality, but now we need some labels. Let's use the clusters found by k-means as labels. This will help visually separate different concentrations of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# colors\n",
    "palette = sns.hls_palette(20, l=.4, s=.9)\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\n",
    "plt.title('t-SNE with Kmeans Labels')\n",
    "plt.savefig(\"Data_Collection/Task_1/improved_cluster_tsne_Task1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labeled plot gives better insight into how the papers are grouped. It is interesting that both k-means and t-SNE are able to agree on certain clusters even though they were ran independetly. The location of each paper on the plot was determined by t-SNE while the label (color) was determined by k-means. If we look at a particular part of the plot where t-SNE has grouped many articles forming a cluster, it is likely that k-means is uniform in the labeling of this cluster (most of the cluster is the same color). This behavior shows that structure within the literature can be observed and measured to some extent. \n",
    "\n",
    "Now there are other cases where the colored labels (k-means) are spread out on the plot (t-SNE). This is a result of t-SNE and k-means finding different connections in the higher dimensional data. The topics of these papers often intersect so it hard to cleanly separate them. This effect can be observed in the formation of subclusters on the plot. These subclusters are a conglomeration of different k-means labels but may share some connection determined by t-SNE.\n",
    "\n",
    "This organization of the data does not act as a simple search engine. The clustering + dimensionality reduction is performed on the mathematical similarities of the publications. As an unsupervised approach, the algorithms may even find connections that were unnaparent to humans. This may highlight hidden shared information and advance further research.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling on Each Cluster\n",
    "\n",
    "Now we will attempt to find the most significant words in each clusters. K-means clustered the articles but did not label the topics. Through topic modeling we will find out what the most important terms for each cluster are. This will add more meaning to the cluster by giving keywords to quickly identify the themes of the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For topic modeling, we will use LDA (Latent Dirichlet Allocation). In LDA, each document can be described by a distribution of topics and each topic can be described by a distribution of words[.](https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[source](https://towardsdatascience.com/latent-dirichlet-allocation-15800c852699)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create 20 vectorizers, one for each of our cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = []\n",
    "    \n",
    "for ii in range(0, 20):\n",
    "    # Creating a vectorizer\n",
    "    vectorizers.append(CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will vectorize the data from each of our clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = []\n",
    "\n",
    "for current_cluster, cvec in enumerate(vectorizers):\n",
    "    try:\n",
    "        vectorized_data.append(cvec.fit_transform(df.loc[df['y'] == current_cluster, 'processed_text']))\n",
    "    except Exception as e:\n",
    "        print(\"Not enough instances in cluster: \" + str(current_cluster))\n",
    "        vectorized_data.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling will be performed through the use of Latent Dirichlet Allocation (LDA). This is a generative statistical model that allows sets of words to be explained by a shared topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics per cluster\n",
    "NUM_TOPICS_PER_CLUSTER = 20\n",
    "\n",
    "lda_models = []\n",
    "for ii in range(0, 20):\n",
    "    # Latent Dirichlet Allocation Model\n",
    "    lda = LatentDirichletAllocation(n_components=NUM_TOPICS_PER_CLUSTER, max_iter=10, learning_method='online',verbose=False, random_state=42)\n",
    "    lda_models.append(lda)\n",
    "    \n",
    "lda_models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each cluster, we had created a correspoding LDA model in the previous step. We will now fit_transform all the LDA models on their respective cluster vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_lda_data = []\n",
    "\n",
    "for current_cluster, lda in enumerate(lda_models):\n",
    "    # print(\"Current Cluster: \" + str(current_cluster))\n",
    "    \n",
    "    if vectorized_data[current_cluster] != None:\n",
    "        clusters_lda_data.append((lda.fit_transform(vectorized_data[current_cluster])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts the keywords from each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for printing keywords for each topic\n",
    "def selected_topics(model, vectorizer, top_n=3):\n",
    "    current_words = []\n",
    "    keywords = []\n",
    "    \n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        words = [(vectorizer.get_feature_names()[i], topic[i]) for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        for word in words:\n",
    "            if word[0] not in current_words:\n",
    "                keywords.append(word)\n",
    "                current_words.append(word[0])\n",
    "                \n",
    "    keywords.sort(key = lambda x: x[1])  \n",
    "    keywords.reverse()\n",
    "    return_values = []\n",
    "    for ii in keywords:\n",
    "        return_values.append(ii[0])\n",
    "    return return_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append list of keywords for a single cluster to 2D list of length NUM_TOPICS_PER_CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = []\n",
    "for current_vectorizer, lda in enumerate(lda_models):\n",
    "    # print(\"Current Cluster: \" + str(current_vectorizer))\n",
    "\n",
    "    if vectorized_data[current_vectorizer] != None:\n",
    "        all_keywords.append(selected_topics(lda, vectorizers[current_vectorizer]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save current outputs to file\n",
    "\n",
    "Re-running some parts of the notebook (especially vectorization and t-SNE) are time intensive tasks. We want to make sure that the important outputs for generating the bokeh plot are saved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('Data_Collection/Task_1/topics.txt','w')\n",
    "\n",
    "count = 0\n",
    "\n",
    "for ii in all_keywords:\n",
    "\n",
    "    if vectorized_data[count] != None:\n",
    "        f.write(', '.join(ii) + \"\\n\")\n",
    "    else:\n",
    "        f.write(\"Not enough instances to be determined. \\n\")\n",
    "        f.write(', '.join(ii) + \"\\n\")\n",
    "    count += 1\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the COVID-19 DataFrame, too large for github\n",
    "pickle.dump(df, open(\"Data_Collection/Task_1/df_covid.p\", \"wb\" ))\n",
    "\n",
    "# save the final t-SNE\n",
    "pickle.dump(X_embedded, open(\"Data_Collection/Task_1/X_embedded.p\", \"wb\" ))\n",
    "\n",
    "# save the labels generate with k-means(20)\n",
    "pickle.dump(y_pred, open(\"Data_Collection/Task_1/y_pred.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify\n",
    "\n",
    "Though arbitrary, after running kmeans, the data is now 'labeled'. This means that we now use supervised learning to see how well the clustering generalizes. This is just one way to evaluate the clustering. If k-means was able to find a meaningful split in the data, it should be possible to train a classifier to predict which cluster a given instance should belong to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print out classification model report\n",
    "def classification_report(model_name, test, pred):\n",
    "    from sklearn.metrics import precision_score, recall_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    \n",
    "    print(model_name, \":\\n\")\n",
    "    print(\"Accuracy Score: \", '{:,.3f}'.format(float(accuracy_score(test, pred)) * 100), \"%\")\n",
    "    print(\"     Precision: \", '{:,.3f}'.format(float(precision_score(test, pred, average='macro')) * 100), \"%\")\n",
    "    print(\"        Recall: \", '{:,.3f}'.format(float(recall_score(test, pred, average='macro')) * 100), \"%\")\n",
    "    print(\"      F1 score: \", '{:,.3f}'.format(float(f1_score(test, pred, average='macro')) * 100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# test set size of 20% of the data and the random seed 42 <3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(),y_pred, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train size:\", len(X_train))\n",
    "print(\"X_test size:\", len(X_test), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision** is ratio of True Positives to True Positives + False Positives. This is the accuracy of positive predictions<br>\n",
    "**Recall** (also known as TPR) measures the ratio of True Positives to True Positives + False Negatives. It measures the ratio of positive instances that are correctly detected by the classifer.<br>\n",
    "**F1 score**  is the harmonic average of the precision and recall. F1 score will only be high if both precision and recall are high\n",
    "\n",
    "#### Cite: [Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow: Second Edition | Aurélien Geron](https://github.com/ageron/handson-ml2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# SGD instance\n",
    "sgd_clf = SGDClassifier(max_iter=10000, tol=1e-3, random_state=42, n_jobs=4)\n",
    "# train SGD\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "\n",
    "# cross validation predictions\n",
    "sgd_pred = cross_val_predict(sgd_clf, X_train, y_train, cv=3, n_jobs=4)\n",
    "\n",
    "# print out the classification report\n",
    "classification_report(\"Stochastic Gradient Descent Report (Training Set)\", y_train, sgd_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test for overfitting, let's see how the model generalizes over the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation predictions\n",
    "sgd_pred = cross_val_predict(sgd_clf, X_test, y_test, cv=3, n_jobs=4)\n",
    "\n",
    "# print out the classification report\n",
    "classification_report(\"Stochastic Gradient Descent Report (Training Set)\", y_test, sgd_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the model can generalize across the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_cv_score = cross_val_score(sgd_clf, X.toarray(), y_pred, cv=10)\n",
    "print(\"Mean cv Score - SGD: {:,.3f}\".format(float(sgd_cv_score.mean()) * 100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the data\n",
    "The previous steps have given us clustering labels and a dataset of papers reduced to two dimensions. By pairing this with Bokeh, we can create an interactive plot of the literature. This should organize the papers such that related publications are in close proximity. To try to undertstand what the similarities may be, we have also performed topic modelling on each cluster of papers in order to pick out the key terms.\n",
    "\n",
    "Bokeh will pair the actual papers with their positions on the t-SNE plot. Through this approach it will be easier to see how papers fit together, allowing for both exploration of the dataset and evaluation of the clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# change into lib directory to load plot python scripts\n",
    "main_path = os.getcwd()\n",
    "lib_path = 'kaggle-resources/'\n",
    "os.chdir(lib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.7 install 'bokeh==1.3.4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries for plot\n",
    "from call_backs import input_callback, selected_code  # file with customJS callbacks for bokeh\n",
    "                                                      # github.com/MaksimEkin/COVID19-Literature-Clustering/blob/master/lib/call_backs.py\n",
    "import bokeh\n",
    "from bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, CustomJS, Slider, TapTool, TextInput\n",
    "from bokeh.palettes import Category20\n",
    "from bokeh.transform import linear_cmap, transform\n",
    "from bokeh.io import output_file, show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import RadioButtonGroup, TextInput, Div, Paragraph\n",
    "from bokeh.layouts import column, widgetbox, row, layout\n",
    "from bokeh.layouts import column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bokeh.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go back\n",
    "os.chdir(main_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Keywords per Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "topic_path = 'Data_Collection/Task_1/topics.txt'\n",
    "with open(topic_path) as f:\n",
    "    topics = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show on notebook\n",
    "output_notebook()\n",
    "# target labels\n",
    "y_labels = y_pred\n",
    "\n",
    "# data sources\n",
    "source = ColumnDataSource(data=dict(\n",
    "    x= X_embedded[:,0], \n",
    "    y= X_embedded[:,1],\n",
    "    x_backup = X_embedded[:,0],\n",
    "    y_backup = X_embedded[:,1],\n",
    "    desc= y_labels, \n",
    "    titles= df['title'],\n",
    "    authors = df['authors'],\n",
    "    journal = df['journal'],\n",
    "    abstract = df['abstract_summary'],\n",
    "    labels = [\"C-\" + str(x) for x in y_labels],\n",
    "    links = df['doi']\n",
    "    ))\n",
    "\n",
    "# hover over information\n",
    "hover = HoverTool(tooltips=[\n",
    "    (\"Title\", \"@titles{safe}\"),\n",
    "    (\"Author(s)\", \"@authors{safe}\"),\n",
    "    (\"Journal\", \"@journal\"),\n",
    "    (\"Abstract\", \"@abstract{safe}\"),\n",
    "    (\"Link\", \"@links\")\n",
    "],\n",
    "point_policy=\"follow_mouse\")\n",
    "\n",
    "# map colors\n",
    "mapper = linear_cmap(field_name='desc', \n",
    "                     palette=Category20[20],\n",
    "                     low=min(y_labels) ,high=max(y_labels))\n",
    "\n",
    "# prepare the figure\n",
    "plot = figure(plot_width=1200, plot_height=850, \n",
    "           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset', 'save', 'tap'], \n",
    "           title=\"Clustering of the COVID-19 Literature with t-SNE and K-Means\", \n",
    "           toolbar_location=\"above\")\n",
    "\n",
    "# plot settings\n",
    "plot.scatter('x', 'y', size=5, \n",
    "          source=source,\n",
    "          fill_color=mapper,\n",
    "          line_alpha=0.3,\n",
    "          line_color=\"black\",\n",
    "          legend = 'labels')\n",
    "plot.legend.background_fill_alpha = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widgets\n",
    "see https://github.com/MaksimEkin/COVID19-Literature-Clustering/issues/6 to solve widget callback attribute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords\n",
    "text_banner = Paragraph(text= 'Keywords: Slide to specific cluster to see the keywords.', height=45)\n",
    "input_callback_1 = input_callback(plot, source, text_banner, topics)\n",
    "\n",
    "# currently selected article\n",
    "div_curr = Div(text=\"\"\"Click on a plot to see the link to the article.\"\"\",height=150)\n",
    "callback_selected = CustomJS(args=dict(source=source, current_selection=div_curr), code=selected_code())\n",
    "taptool = plot.select(type=TapTool)\n",
    "taptool.callback = callback_selected\n",
    "\n",
    "# WIDGETS\n",
    "slider = Slider(start=0, end=20, value=20, step=1, title=\"Cluster #\", callback=input_callback_1 )\n",
    "keyword = TextInput(title=\"Search:\", callback=input_callback_1)\n",
    "\n",
    "\n",
    "# pass call back arguments\n",
    "input_callback_1.args[\"text\"] = keyword\n",
    "input_callback_1.args[\"slider\"] = slider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STYLE\n",
    "slider.sizing_mode = \"stretch_width\"\n",
    "slider.margin=15\n",
    "\n",
    "keyword.sizing_mode = \"scale_both\"\n",
    "keyword.margin=15\n",
    "\n",
    "div_curr.style={'color': '#BF0A30', 'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\n",
    "div_curr.sizing_mode = \"scale_both\"\n",
    "div_curr.margin = 20\n",
    "\n",
    "text_banner.style={'color': '#0269A4', 'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\n",
    "text_banner.sizing_mode = \"scale_both\"\n",
    "text_banner.margin = 20\n",
    "\n",
    "plot.sizing_mode = \"scale_both\"\n",
    "plot.margin = 5\n",
    "\n",
    "r = row(div_curr,text_banner)\n",
    "r.sizing_mode = \"stretch_width\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAYOUT OF THE PAGE\n",
    "l = layout([\n",
    "    [slider, keyword],\n",
    "    [text_banner],\n",
    "    [div_curr],\n",
    "    [plot],\n",
    "])\n",
    "l.sizing_mode = \"scale_both\"\n",
    "\n",
    "# show\n",
    "output_file('Data_Collection/Task_1/t-sne_covid-19_interactive_Task1.html')\n",
    "show(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use the Plot?\n",
    "\n",
    "Each dot on the plot represents a research article. They are color-coded by the cluster number that kmeans had found. By default no cluster is selected and all the clusters are displayed. To select a distinct cluster, set the slider to the desired cluster number. When an individual cluster is selected, a list of keywords found in that cluster will be displayed above the plot.You can also search for articles by specific keyword, either within an individual cluster or within all. Let's go through an example of how the tool can be used. \n",
    "\n",
    "**Note, This example is pulled from the full plot, available at:** <br>\n",
    "https://maksimekin.github.io/COVID19-Literature-Clustering/plots/t-sne_covid-19_interactive.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's check out the different clusters and see if anything interesting stands out. The easiest way to get a general idea of what is going on in each cluster is to look to the keywords for each cluster (the ones we identified through topic modeling). For the sake of example, let's say we see some keywords that interest us in cluster #9. This cluster seems to center around livestock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"kaggle-resources/cluster_9_keywords.PNG\", width=1170, height=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "And here is what the actual cluster looks like, zoomed in on the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='kaggle-resources/cluster_9.PNG', width=420, height=375)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say we interested specifically in cattle. Let's use the keyword search to filter for papers that specifically mention cattle. This is what the same cluster will look like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='kaggle-resources/cluster_9_cattle.PNG', width=420, height=375)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shown articles are now a little thinner, showing only papers that are part of the cluster and that contain the keyword 'cattle'. The search can also be expanded to the entire dataset by setting the # of clusters to the default value of 20. Hovering over any article will give basic information like title, author, and abstract. Clicking on an article will pin that article above the plot and give a URL that will direct the user to the full paper. Here's what clicking an article looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='Data_Collection/kaggle-resources/selected_paper.PNG', width=600, height=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "*Note: We will be using the [full plot](https://maksimekin.github.io/COVID19-Literature-Clustering/plots/t-sne_covid-19_interactive.html) for these examples*\n",
    "\n",
    "The plot can be used to quickly find many publications on a similar topic. For example, let's say we're interested in finding information on masks and their effectiveness. To do this we can either try to find a cluster with a keyword such as \"mask\" or we can search for the term directly and try to identify which cluster it most closely relates to. \n",
    "\n",
    "If you search for \"mask\", a dark green cluster (#12) is well represented. It would be wise to start the looking there. A quick scan of the titles indicates that these publications mainly address different masks and their uses for healthcare workers. \n",
    "\n",
    "Searching for a single key term may cause you to inadvertenly filter out highly similar papers that use different phrasing. After an initial examination, clear the search term and explore the whole cluster by adjusting the slider. In this case, we would set the slider to 12 and look for more interesting papers in the entire cluster.\n",
    "\n",
    "Now we will explore a couple task-oriented questions.\n",
    "\n",
    "## What do we know about diagnostics and surveillance?\n",
    "\n",
    "### Diagnostics\n",
    "- To start, we first searched one of the keyterms from this question (\"diagnostic\")\n",
    "- For \"diagnostic\", the brown cluster (#17) stood out immediately\n",
    "- Next we removed the search term and adjusted the slider to 17\n",
    "- Inside cluster 17 there exists a main cluster and a smaller, denser sub-cluster just off to the right of the main one. Both this main cluster and the sub-cluster seem to contain useful information on diagnosing viral infections. Just quickly skimming through the publications, we saw papers such as:\n",
    "\n",
    "    - [Evaluation of fast-track diagnostics and TaqMan array card real-time PCR assays for the detection of respiratory pathogens](https://www.sciencedirect.com/science/article/pii/S0167701214002991?via%3Dihub) \n",
    "    - [Advances in the diagnosis of respiratory virus infections](https://www.sciencedirect.com/science/article/pii/0928019796002103?via%3Dihub) \n",
    "\n",
    "### Surveillance \n",
    "- Next, we returned to the full plot by adjusting the slider to cluster 20 (clusters range from 0 to 19; 20 plots all)\n",
    "- We then repeated the search process with keyterm \"surveillance\"\n",
    "- The dominant cluster for this search term was cluster #8. After clearing the search box and adjusting the slider to 8, we looked at some of the titles and abstracts in this cluster\n",
    "- A couple that stood out: \n",
    "\n",
    "    - [Surveillance strategy for early detection of unusual infectious disease events](https://www.sciencedirect.com/science/article/pii/S1879625713000217?via%3Dihub)  \n",
    "    - [Communicable Disease Surveillance Ethics in the Age of Big Data and New Technology](https://link.springer.com/article/10.1007/s41649-019-00087-1)\n",
    "\n",
    "<br>\n",
    "## What do we know about vaccines and therapeutics?\n",
    "\n",
    "### Vaccine\n",
    "- To identify which cluster to examine first for papers on vaccines, we used a different strategy than before\n",
    "- By examinining the keywords generated through topic modeling for each cluster, we found that cluster #15 had related keywords like \"vaccine\", \"serum\", and \"delivery\"\n",
    "- To further verify, we also searched for \"vaccine\" within the whole dataset and found that cluster #15 was well represented\n",
    "- While exploring this cluster we ran into an issue of overrepresentation - many publications focused on vaccines that seemed unrelated. To remedy this we searched for \"corona\" within the cluster\n",
    "- This narrowed down the publications and made it easier to look for interesting papers. Here are a few that we found:\n",
    "\n",
    "    - [Development of next-generation respiratory virus vaccines through targeted modifications to viral immunomodulatory genes](https://www.tandfonline.com/doi/full/10.1586/14760584.2015.1095096)\n",
    "    - [Intranasal DNA Vaccine for Protection against Respiratory Infectious Diseases: The Delivery Perspectives](https://www.mdpi.com/1999-4923/6/3/378)\n",
    "\n",
    "### Therapeutics\n",
    "- Cluster #1 has more information on vaccine/therapeutics\n",
    "- Specifically, there is a lot of information on coronaviruses (the top keyword in the cluster is MERS)\n",
    "- A few interesting papers from cluster #1:\n",
    "\n",
    "    - [A safe and convenient pseudovirus-based inhibition assay to detect neutralizing antibodies and screen for viral entry inhibitors against the novel human coronavirus MERS-CoV](https://virologyj.biomedcentral.com/articles/10.1186/1743-422X-10-266)\n",
    "    - [Advances in MERS-CoV Vaccines and Therapeutics Based on the Receptor-Binding Domain](https://www.mdpi.com/1999-4915/11/1/60)\n",
    "    \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this project, we have attempted to cluster published literature on COVID-19 and reduce the dimensionality of the dataset for visualization purposes. This has allowed for an interactive scatter plot of papers related to COVID-19, in which material of similar theme is grouped together. Grouping the literature in this way allows for professionals to quickly find material related to a central topic. Instead of having to manually search for related work, every publication is connected to a larger topic cluster. The clustering of the data was done through k-means on a pre-processed, vectorized version of the literature’s body text. As k-means simply split the data into clusters, topic modeling through LDA was performed to identify keywords. This gave the topics that were prevalent in each of the clusters. Both the clusters and keywords are found through unsupervised learning models and can be useful in revealing patterns that humans may not have even thought about. In no part of this project did we have to manually organize the papers: the results are due to latent connections in the data. \n",
    " \n",
    "K-means (represented by colors) and t-SNE (represented by points) were able to independently find clusters, showing that relationships between papers can be identified and measured. Papers written on highly similar topics are typically near each other on the plot and bear the same k-means label. However, due to the complexity of the dataset, k-means and t-SNE will sometimes arrive at different decisions. The topics of much of the given literature are continuous and will not have a concrete decision boundary. This means that k-means and t-SNE can find different similarities to group the papers by. In these conditions, our approach performs quite well.\n",
    "\n",
    "As this is an unsupervised learning problem, the evaluation of our work was not an exact science. First, the plot was examined to assert that clusters were actually being formed. After being convinced of this, we examined the titles/abstracts of some of the papers in different clusters. For the most part, similar research areas were clustered. Our last evaluation method was classification. By training a classification model with the k-means labels and then testing it on a separate subset of the data, we could see that the clustering was not completely arbitrary as the classifier performed well. \n",
    "\n",
    "Our manual inspection of the documents was quite limited, as neither of the authors are qualified to assess the meaning of the literature. Even so, it was apparent that articles on key topics could be easily found in close proximity to each other. For example, searching for 'mask' can reveal a sub cluster of papers that evaluate the efficacy of masks.  We believe that health professionals can use this tool to find real links in the texts. By organizing the literature, qualified people can quickly find related publications that answer the task questions. This project can further be improved by abstracting the underlying data analysis techniques as described in this notebook to develop a user interface/tool that presents the related articles in a user-friendly manner.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some areas we thought were great:** (pros)\n",
    "- The tool is saved as an html file. It can be downloaded and used locally/offline.\n",
    "- It is portable/mobile, easily deployable, and failover safe; the risk of denial of service in the case of emergencies such as the loss of network connection is mitigated\n",
    "- Dimensionality reduction allows for the whole dataset to be easily accessible. The papers are all up on the plot and can be quickly examined by hovering over them. If the abstract seems interesting, the user can click on the point to bring up a text box with more information that will contain a link to the full paper\n",
    "- Patterns in the data found through clustering/dimensionality reduction may not be readily apparent to researchers. These unsupervised techniques can show humans hidden connnections in the literature\n",
    "- If the topics/clusters are not narrow enough for the user, a search for a key term that will only bring up papers that contain the search term. Search can be performed inside of a selected cluster or the entire dataset if preferred. This increases the flexibility of how patterns can be discovered.\n",
    "- A surface level examination of the plot showed some very interesting organization of the data. For example, one subcluster consisted of papers that tried to determine the efficacy of masks in preventing the spread of COVID-19.\n",
    "- Once the models are trained, the results can be generated in constant time.\n",
    "- This work can be easily replicated and modified as needed, serving as a foundation for future projects.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Future thoughts to consider:** (cons)\n",
    "- Possible false positives, difficult to draw an exact line between subjects\n",
    "- K-means and t-SNE are unsupervised approaches that will not necessarily group instances in a predictable way. Due to their unsupervised nature, there is no 'right answer' for how the papers should be clustered. This could be difficult to debug if problems arise.\n",
    "- Loss of foreign language papers. This leads to the loss of experience from different geographic locations on dealing with COVID-19\n",
    "- The algorithms used in this notebook are stochastic so the results may vary depending on the random state. In this notebook all of the algorithms are set to random state 42 (the meaning of life) to ensure reproducible results\n",
    "- Long run time to train models on large dataset of literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation/Sources\n",
    "\n",
    "Kaggle Submission: [COVID-19 Literature Clustering | Kaggle](https://www.kaggle.com/maksimeren/covid-19-literature-clustering#Unsupervised-Learning:-Clustering-with-K-Means)\n",
    " \n",
    " ```\n",
    "@inproceedings{Raff2020,\n",
    "\tauthor = {Raff, Edward and Nicholas, Charles and McLean, Mark},\n",
    "\tbooktitle = {The Thirty-Fourth AAAI Conference on Artificial Intelligence},\n",
    "\ttitle = {{A New Burrows Wheeler Transform Markov Distance}},\n",
    "\turl = {http://arxiv.org/abs/1912.13046},\n",
    "\tyear = {2020},\n",
    "}\n",
    "```\n",
    "```\n",
    "@misc{Kaggle,\n",
    "\tauthor = {Kaggle},\n",
    "\ttitle = {COVID-19 Open Research Dataset Challenge (CORD-19)},\n",
    "\tyear = {2020},\n",
    "\tmonth = {March},\n",
    "\tnote = {Allen Institute for AI in partnership with the Chan Zuckerberg Initiative, Georgetown University’s Center for   Security and Emerging Technology, Microsoft Research, and the National Library of Medicine - National Institutes of Health, in coordination with The White House Office of Science and Technology Policy.},\n",
    "\thowpublished = {\\url{https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge}}\n",
    "}\n",
    "```\n",
    "```\n",
    "@inproceedings{Shakespeare,\n",
    "\tauthor = {Nicholas, Charles},\n",
    "\ttitle = {Mr. Shakespeare, Meet Mr. Tucker},\n",
    "\tbooktitle = {High Performance Computing and Data Analytics Workshop},\n",
    "\tyear = {2019},\n",
    "\tmonth = {September},\n",
    "\tlocation = { Linthicum Heights, MD, USA},\n",
    "}\n",
    "```\n",
    "```\n",
    "@inproceedings{raff_lzjd_2017,\n",
    "\tauthor = {Raff, Edward and Nicholas, Charles},\n",
    "\ttitle = {An Alternative to NCD for Large Sequences, Lempel-Ziv Jaccard Distance},\n",
    "\tbooktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},\n",
    "\tseries = {KDD '17},\n",
    "\tyear = {2017},\n",
    "\tisbn = {978-1-4503-4887-4},\n",
    "\tlocation = {Halifax, NS, Canada},\n",
    "\tpages = {1007--1015},\n",
    "\tnumpages = {9},\n",
    "\turl = {http://doi.acm.org/10.1145/3097983.3098111},\n",
    "\tdoi = {10.1145/3097983.3098111},\n",
    "\tacmid = {3098111},\n",
    "\tpublisher = {ACM},\n",
    "\taddress = {New York, NY, USA},\n",
    "\tkeywords = {cyber security, jaccard similarity, lempel-ziv, malware classification, normalized compression distance},\n",
    "}\n",
    "```\n",
    "```\n",
    "@inproceedings{ML_Book,\n",
    "    author = {Aurélien Geron},\n",
    "    title = {Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow},\n",
    "    pages = {91, 233},\n",
    "    series = {2},\n",
    "    year = {2019},\n",
    "    isbn = {978-1-492-03264-9},\n",
    "    publisher = {O'Reilly},\n",
    "    copyright = {Kiwisoft S.A.S},\n",
    "}\n",
    "```\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Thank you for looking at our notebook. We greatly appreciate tips, suggestions, and upvotes :)** <br>\n",
    "**If you would like to check out the interactive plot with the full dataset, please visit:**\n",
    "\n",
    "https://maksimekin.github.io/COVID19-Literature-Clustering/plots/t-sne_covid-19_interactive.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
